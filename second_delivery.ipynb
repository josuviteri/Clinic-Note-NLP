{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f3541e",
   "metadata": {},
   "source": [
    "### Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b39a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9399869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/MTS-Dialog-TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af598ae4",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95961e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'section_header', 'section_text', 'dialogue']\n",
      "(1201, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>section_header</th>\n",
       "      <th>section_text</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>The patient is a 76-year-old white female who ...</td>\n",
       "      <td>Doctor: What brings you back into the clinic t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>The patient is a 25-year-old right-handed Cauc...</td>\n",
       "      <td>Doctor: How're you feeling today?  \\r\\nPatient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GENHX</td>\n",
       "      <td>This is a 22-year-old female, who presented to...</td>\n",
       "      <td>Doctor: Hello, miss. What is the reason for yo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID section_header                                       section_text  \\\n",
       "0   0          GENHX  The patient is a 76-year-old white female who ...   \n",
       "1   1          GENHX  The patient is a 25-year-old right-handed Cauc...   \n",
       "2   2          GENHX  This is a 22-year-old female, who presented to...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  Doctor: What brings you back into the clinic t...  \n",
       "1  Doctor: How're you feeling today?  \\r\\nPatient...  \n",
       "2  Doctor: Hello, miss. What is the reason for yo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df.columns.tolist())\n",
    "print(df.shape)\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ec8dc7",
   "metadata": {},
   "source": [
    "Clinical dialog → preserve: speaker markings (Doctor:, Patient:) and numeric/measurement tokens (e.g., “76-year-old”, “BP: 130/80”), but normalize dates/IDs and check for PHI (dataset is supposed to be deidentified, but verify).\n",
    "\n",
    "Recommended cleaning pipeline (reasoning inlined):\n",
    "\n",
    "Case folding: lowercase for classical tokenizers / non-cased models. For BERT variants, use the model tokenizer (no lowercasing if using a cased model).\n",
    "\n",
    "Normalize whitespace & unicode (NFKC).\n",
    "\n",
    "Preserve speaker tokens — convert “Doctor: … Patient: …” to explicit tokens like <DOC> / <PAT>.\n",
    "\n",
    "Normalize numbers (optional): you may map digits to a special token <NUM> or keep them (I recommend keeping them for clinical features like dosages).\n",
    "\n",
    "Do NOT aggressively remove punctuation — punctuation carries clinical meaning (e.g., “+”, “/”, “mg”).\n",
    "\n",
    "Remove obvious transcription artifacts (e.g., “um”, “uh” — optionally).\n",
    "\n",
    "Lemmatization rather than stemming (better for clinical semantics).\n",
    "\n",
    "Use domain / science tokenizers (scispaCy / clinical spaCy tokenizers are recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b61a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    # unicode normalize\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # preserve speaker markers but normalize them:\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' <DOC> ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' <PAT> ', s, flags=re.I)\n",
    "    # collapse whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s.lower()  # or don't lowercase if feeding into a cased BERT later\n",
    "\n",
    "df['dialog_clean'] = df['dialogue'].apply(normalize_text)\n",
    "df['section_text_clean'] = df['section_text'].apply(normalize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bbcdec",
   "metadata": {},
   "source": [
    "## 3. Tokenization, lemmatization, stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81669d",
   "metadata": {},
   "source": [
    "Recommendations: usage spaCy (or scispaCy) for tokenization + lemmatization. scispaCy models are tuned for biomedical language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# for clinical/biomedical prefer scispaCy if possible:\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # or \"en_core_web_sm\" if scispaCy not available\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [tok.lemma_ for tok in doc if not tok.is_space]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['dialog_lemma'] = df['dialog_clean'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691d69a",
   "metadata": {},
   "source": [
    "Stemming: don't prefer for this task (it destroys clinical term forms). If you need a light form, use lemmatization.\n",
    "\n",
    "Tokenization for models:\n",
    "\n",
    "For BERT / ClinicalBERT use the HuggingFace tokenizer for that model (AutoTokenizer.from_pretrained(...)) — do not pass spaCy-tokenized text into the model; pass raw strings to the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1e5bf",
   "metadata": {},
   "source": [
    "## 4. Usage of Non textual data (section_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd229b",
   "metadata": {},
   "source": [
    "Section_header is essentially the clinical section label (e.g. GENHX, CC, PASTMEDICALHX, MEDICATIONS). Options:\n",
    "\n",
    "As supervised target grouping: if you want to train a model per section type, filter rows by section_header.\n",
    "\n",
    "As a categorical feature: one-hot encode (or embedding) and concatenate to text embeddings.\n",
    "\n",
    "As a conditioning prompt: prefix the input to your generation model with the header: \"[SECTION=MEDICATIONS] Doctor: ... Patient: ...\". Conditioning helps guide generation to the right section style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b3e13",
   "metadata": {},
   "source": [
    "## 5. Traditional Text Vectors: BoW and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c49e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Bag-of-words (unigrams + bigrams)\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_bow = cv.fit_transform(df['dialog_lemma'])\n",
    "\n",
    "# TF-IDF\n",
    "tfv = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_tfidf = tfv.fit_transform(df['dialog_lemma'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51715a42",
   "metadata": {},
   "source": [
    "Tuning tips:\n",
    "\n",
    "max_df / min_df to remove overly common/rare tokens\n",
    "\n",
    "ngram_range=(1,2) often helps for short dialogues\n",
    "\n",
    "Keep max_features small for fast baselines, larger for later models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa70f44",
   "metadata": {},
   "source": [
    "## 6. Non contextual word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e446e3",
   "metadata": {},
   "source": [
    "Word2Vec, GloVe, FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74327708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# options available via gensim:\n",
    "w2v = api.load(\"word2vec-google-news-300\")          # Word2Vec\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")        # GloVe\n",
    "ft = api.load(\"fasttext-wiki-news-subwords-300\")   # FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1753a81",
   "metadata": {},
   "source": [
    "Sentence / dialogue vectorization: average token vectors, or compute TF-IDF-weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1089b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_embedding(text, model):\n",
    "    words = [w for w in text.split() if w in model.key_to_index]\n",
    "    if not words:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model[w] for w in words], axis=0)\n",
    "\n",
    "df['glove_avg'] = df['dialog_lemma'].apply(lambda t: avg_embedding(t, glove))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53939aba",
   "metadata": {},
   "source": [
    "TF-IDF weighted average (better than plain mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5136bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute tfidf weights for tokens then weighted sum of embeddings\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_tfidf_tokens = vectorizer.fit_transform(df['dialog_lemma'])  # sparse matrix\n",
    "feature_index = {v: i for i, v in enumerate(vectorizer.get_feature_names_out())}\n",
    "\n",
    "def tfidf_weighted_embedding(text, model, tfidf_vector):\n",
    "    tokens = text.split()\n",
    "    vec = np.zeros(model.vector_size)\n",
    "    weight_sum = 0.0\n",
    "    for t in tokens:\n",
    "        if t in model.key_to_index and t in feature_index:\n",
    "            w = tfidf_vector[0, feature_index[t]]\n",
    "            vec += w * model[t]\n",
    "            weight_sum += w\n",
    "    return vec / (weight_sum + 1e-9)\n",
    "\n",
    "# example for a single row:\n",
    "sample_vec = tfidf_weighted_embedding(df.loc[0,'dialog_lemma'], glove, X_tfidf_tokens[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55caae",
   "metadata": {},
   "source": [
    "## 7. Contextual embeddings -- sentence / dialog level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3df9df",
   "metadata": {},
   "source": [
    "Two main approaches: ELMo (TF Hub / AllenNLP) and BERT family. For clinical, use Bio/Clinical variants (BioBERT, BioClinicalBERT, ClinicalBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883eb3b5",
   "metadata": {},
   "source": [
    "ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elmo implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdaf419",
   "metadata": {},
   "source": [
    "BERT / ClinicalBERT (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elmo implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_nlp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
