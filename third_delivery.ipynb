{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046519d3",
   "metadata": {},
   "source": [
    "Este proyecto tiene como objetivo desarrollar un modelo de procesamiento\n",
    "de lenguaje natural (NLP) capaz de generar resúmenes clínicos automáticos\n",
    "a partir de un dataset de alrededor de 1700 conversaciones entre doctores y\n",
    "sus pacientes, junto con los respectivos resúmenes y anotaciones.\n",
    "\n",
    "    Los objetivos de esta entrega 3 son:\n",
    "\n",
    "    1. Correciones de la entrega 2:\n",
    "\n",
    "Por ejemplo, ¿por qué lematizais? ¿Habéis analizado qué pasa con los word embeddings lematizado vs no-lematizado? Para embeddings la recomendación es no lematizar y con tf-idf habría que analizarlo con la tarea que queráis resolver. Además, ¿qué son lo que vosotros denomiáis tokens? Porque de 4367 palabras únicas no sé cómo salen 173,867 tokens.\n",
    "\n",
    "Por otro lado, el análisis de longitud está muy bien pero lo hacéis a nivel de palabra, no de token. De cara a siguientes entregas hacerlo también a nivel de token para ver si un BIOBert por ejemplo tiene contexto suficiente.\n",
    "\n",
    "\n",
    "\n",
    "    2. Definición de la tarea:\n",
    "Generación de resúmenes y clasificación del diagnóstico.\n",
    "\n",
    "    3. Tareas Específicas de la entrega 3\n",
    "\n",
    "Para ello, se deberán usar técnicas tanto de Shallow ML (o ML tradicional), como algunos de los modelos de CNNs o Redes Recurrentes que hemos visto en clase.\n",
    "\n",
    "Comparar experimentos usando distintas métricas y optimizar los hiperparámetros.\n",
    "\n",
    "Usar atención, combinar features (no creo que aplique a nuestro problema)\n",
    "\n",
    "    Mínimos exigibles:\n",
    "Dos técnicas de Shallow Learning utilizando técnicas de representación dispersa/sparse.\n",
    "\n",
    "Dos técnicas de Deep Learning comparando diferentes tipos de embeddings y fine-tuneandolos dependiendo del caso. Ejemplos:\n",
    "\n",
    "Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec “from scratch”\n",
    "\n",
    "Embedding fine-tuneado durante el entrenamiento vs Embedding inicializado\n",
    "\n",
    "Comparar al menos dos formas de embeddings de cada tipo:\n",
    "\n",
    "Tradicionales: e.g., Bag-of-Words, TF-IDF, etc.\n",
    "\n",
    "Semánticos No-Contextuales: e.g., Glove, FastText, Word2Vec, etc.\n",
    "\n",
    "Contextuales: e.g., ELMo, BERT, Modelos pre-entrenados de Hugging-Face, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f5313",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Instalación de librerías\n",
    "%pip install -q spacy gensim transformers torch tensorflow tensorflow-hub seaborn matplotlib scikit-learn\n",
    "\n",
    "\n",
    "# Descargar modelo de idioma de spaCy\n",
    "%pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz\n",
    "\n",
    "# Imports\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP y embeddings\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18241918",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(s, lowercase=True):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    # Normalizar unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # Marcadores de quién habla\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' <DOC> ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' <PAT> ', s, flags=re.I)\n",
    "    # Espacios\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # Lowercase opcional\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    return s\n",
    "\n",
    "# Versión para ELMo (lowercase)\n",
    "df['dialog_clean'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=True))\n",
    "\n",
    "# Versión para BIO/ClinicalBERT (manteniendo mayúsculas)\n",
    "df['dialog_clean_clinicBERT'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=False))\n",
    "\n",
    "# Los resúmenes\n",
    "df['section_text_clean'] = df['section_text'].apply(lambda x: normalize_text(x, lowercase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc32b13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Embeddings (sin lemmatization)\n",
    "\n",
    "w2v = api.load(\"word2vec-google-news-300\")          # Word2Vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb8956d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Obtener vocabulario del dataset\n",
    "all_tokens = []\n",
    "for text in df['dialog_lemma']:\n",
    "    if pd.notna(text):\n",
    "        all_tokens.extend(text.split())\n",
    "\n",
    "vocab = set(all_tokens)\n",
    "vocab_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"\\nVocabulario total: {len(vocab):,} palabras únicas\")\n",
    "print(f\"Total de tokens: {len(all_tokens):,}\")\n",
    "\n",
    "# Análisis para cada modelo\n",
    "models = {\n",
    "    'Word2Vec (Google News)': w2v,\n",
    "    'GloVe (Wiki Gigaword)': glove,\n",
    "    'FastText (Wiki News)': ft\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    \n",
    "    found_words = [w for w in vocab if w in model.key_to_index]\n",
    "    missing_words = [w for w in vocab if w not in model.key_to_index]\n",
    "    \n",
    "    found_tokens = sum(vocab_freq[w] for w in found_words)\n",
    "    total_tokens = sum(vocab_freq.values())\n",
    "    \n",
    "    coverage_vocab = len(found_words) / len(vocab) * 100\n",
    "    coverage_tokens = found_tokens / total_tokens * 100\n",
    "    \n",
    "    print(f\"Palabras encontradas: {len(found_words):,}/{len(vocab):,} ({coverage_vocab:.2f}%)\")\n",
    "    print(f\"Tokens cubiertos: {found_tokens:,}/{total_tokens:,} ({coverage_tokens:.2f}%)\")\n",
    "    \n",
    "    missing_freq = {w: vocab_freq[w] for w in missing_words}\n",
    "    top_missing = sorted(missing_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\nTop 10 palabras NO encontradas (más frecuentes):\")\n",
    "    for word, freq in top_missing:\n",
    "        print(f\"  {word:20s}: {freq:5d} ocurrencias\")\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Cobertura Vocabulario (%)': round(coverage_vocab, 2),\n",
    "        'Cobertura Tokens (%)': round(coverage_tokens, 2),\n",
    "        'Palabras Encontradas': len(found_words),\n",
    "        'Palabras No Encontradas': len(missing_words)\n",
    "    })\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\nTABLA COMPARATIVA DE COBERTURA\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
