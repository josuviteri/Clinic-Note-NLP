{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "046519d3",
   "metadata": {},
   "source": [
    "Este proyecto tiene como objetivo desarrollar un modelo de procesamiento\n",
    "de lenguaje natural (NLP) capaz de generar resúmenes clínicos automáticos\n",
    "a partir de un dataset de alrededor de 1700 conversaciones entre doctores y\n",
    "sus pacientes, junto con los respectivos resúmenes y anotaciones.\n",
    "\n",
    "    Los objetivos de esta entrega 3 son:\n",
    "\n",
    "    1. Correciones de la entrega 2:\n",
    "\n",
    "Por ejemplo, ¿por qué lematizais? ¿Habéis analizado qué pasa con los word embeddings lematizado vs no-lematizado? Para embeddings la recomendación es no lematizar y con tf-idf habría que analizarlo con la tarea que queráis resolver. Además, ¿qué son lo que vosotros denomiáis tokens? Porque de 4367 palabras únicas no sé cómo salen 173,867 tokens.\n",
    "\n",
    "Por otro lado, el análisis de longitud está muy bien pero lo hacéis a nivel de palabra, no de token. De cara a siguientes entregas hacerlo también a nivel de token para ver si un BIOBert por ejemplo tiene contexto suficiente.\n",
    "\n",
    "\n",
    "\n",
    "    2. Definición de la tarea:\n",
    "Generación de resúmenes y clasificación del diagnóstico.\n",
    "\n",
    "    3. Tareas Específicas de la entrega 3\n",
    "\n",
    "Para ello, se deberán usar técnicas tanto de Shallow ML (o ML tradicional), como algunos de los modelos de CNNs o Redes Recurrentes que hemos visto en clase.\n",
    "\n",
    "Comparar experimentos usando distintas métricas y optimizar los hiperparámetros.\n",
    "\n",
    "Usar atención, combinar features (no creo que aplique a nuestro problema)\n",
    "\n",
    "    Mínimos exigibles:\n",
    "Dos técnicas de Shallow Learning utilizando técnicas de representación dispersa/sparse.\n",
    "\n",
    "Dos técnicas de Deep Learning comparando diferentes tipos de embeddings y fine-tuneandolos dependiendo del caso. Ejemplos:\n",
    "\n",
    "Word2Vec congelado vs Word2Vec fine-tuneado vs Word2Vec “from scratch”\n",
    "\n",
    "Embedding fine-tuneado durante el entrenamiento vs Embedding inicializado\n",
    "\n",
    "Comparar al menos dos formas de embeddings de cada tipo:\n",
    "\n",
    "Tradicionales: e.g., Bag-of-Words, TF-IDF, etc.\n",
    "\n",
    "Semánticos No-Contextuales: e.g., Glove, FastText, Word2Vec, etc.\n",
    "\n",
    "Contextuales: e.g., ELMo, BERT, Modelos pre-entrenados de Hugging-Face, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df727534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "\n",
    "%pip install -q spacy gensim transformers torch tensorflow tensorflow-hub seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6eafb268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz\n",
      "  Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz (119.1 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting spacy<3.8.0,>=3.7.4 (from en_core_sci_md==0.5.4)\n",
      "  Using cached spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.0.12)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.4.3)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.12.4)\n",
      "Requirement already satisfied: jinja2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2025.11.12)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (14.2.0)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.20.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (7.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (3.0.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4)\n",
      "  Using cached marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (2.0.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.4->en_core_sci_md==0.5.4) (0.1.2)\n",
      "Using cached spacy-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Using cached thinc-8.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (865 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached blis-0.7.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached marisa_trie-1.3.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Building wheels for collected packages: en_core_sci_md\n",
      "  Building wheel for en_core_sci_md (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en_core_sci_md: filename=en_core_sci_md-0.5.4-py3-none-any.whl size=119157960 sha256=6dbea7f5087e5595b02c40a8a4590f764b12440fd1945973fec9f853293e6f10\n",
      "  Stored in directory: /home/bnnyrabbit/.cache/pip/wheels/1a/02/d9/4d4bda80f6b73c02ed057d4b3b99abaff286924a80b53c9d68\n",
      "Successfully built en_core_sci_md\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy, marisa-trie, language-data, blis, typer, langcodes, thinc, spacy, en_core_sci_md\n",
      "  Attempting uninstall: blis\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: blis 1.3.3\n",
      "    Uninstalling blis-1.3.3:\n",
      "      Successfully uninstalled blis-1.3.3\n",
      "  Attempting uninstall: thinc\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: thinc 8.3.10\n",
      "    Uninstalling thinc-8.3.10:\n",
      "      Successfully uninstalled thinc-8.3.10\n",
      "  Attempting uninstall: spacy\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: spacy 3.8.11\n",
      "    Uninstalling spacy-3.8.11:\n",
      "      Successfully uninstalled spacy-3.8.11\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed blis-0.7.11 en_core_sci_md-0.5.4 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 numpy spacy-3.7.5 thinc-8.2.5 typer-0.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Descargar modelo de idioma de spaCy\n",
    "\n",
    "%pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_md-0.5.4.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "462f5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-18 20:03:31.589629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages/tensorflow_hub/__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Imports\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP y embeddings\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f527bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/MTS-Dialog-TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18241918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s, lowercase=True):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    # Normalizar unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # Marcadores de quién habla\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' <DOC> ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' <PAT> ', s, flags=re.I)\n",
    "    # Espacios\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # Lowercase opcional\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    return s\n",
    "\n",
    "# Versión para ELMo (lowercase)\n",
    "df['dialog_clean'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=True))\n",
    "\n",
    "# Versión para BIO/ClinicalBERT (manteniendo mayúsculas)\n",
    "df['dialog_clean_clinicBERT'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=False))\n",
    "\n",
    "# Los resúmenes\n",
    "df['section_text_clean'] = df['section_text'].apply(lambda x: normalize_text(x, lowercase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bc32b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Embeddings (sin lemmatization)\n",
    "\n",
    "w2v = api.load(\"word2vec-google-news-300\")          # Word2Vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e6039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-300\")        # GloVe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca6d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "ft = api.load(\"fasttext-wiki-news-subwords-300\")   # FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb8956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulario total: 8,899 palabras únicas\n",
      "Total de tokens: 126,935\n",
      "\n",
      "Modelo: Word2Vec (Google News)\n",
      "Palabras encontradas: 4,295/8,899 (48.26%)\n",
      "Tokens cubiertos: 85,654/126,935 (67.48%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <doc>               :  5810 ocurrencias\n",
      "  <pat>               :  4895 ocurrencias\n",
      "  a                   :  2101 ocurrencias\n",
      "  to                  :  2080 ocurrencias\n",
      "  and                 :  2024 ocurrencias\n",
      "  of                  :  1580 ocurrencias\n",
      "  yes,                :   577 ocurrencias\n",
      "  guest_family:       :   559 ocurrencias\n",
      "  no,                 :   509 ocurrencias\n",
      "  i'm                 :   441 ocurrencias\n",
      "\n",
      "Modelo: GloVe (Wiki Gigaword)\n",
      "Palabras encontradas: 4,233/8,899 (47.57%)\n",
      "Tokens cubiertos: 90,800/126,935 (71.53%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <doc>               :  5810 ocurrencias\n",
      "  <pat>               :  4895 ocurrencias\n",
      "  yes,                :   577 ocurrencias\n",
      "  guest_family:       :   559 ocurrencias\n",
      "  no,                 :   509 ocurrencias\n",
      "  i'm                 :   441 ocurrencias\n",
      "  it's                :   378 ocurrencias\n",
      "  yeah,               :   340 ocurrencias\n",
      "  okay.               :   335 ocurrencias\n",
      "  that's              :   332 ocurrencias\n",
      "\n",
      "Modelo: FastText (Wiki News)\n",
      "Palabras encontradas: 5,284/8,899 (59.38%)\n",
      "Tokens cubiertos: 98,911/126,935 (77.92%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <doc>               :  5810 ocurrencias\n",
      "  <pat>               :  4895 ocurrencias\n",
      "  yes,                :   577 ocurrencias\n",
      "  guest_family:       :   559 ocurrencias\n",
      "  no,                 :   509 ocurrencias\n",
      "  i'm                 :   441 ocurrencias\n",
      "  it's                :   378 ocurrencias\n",
      "  yeah,               :   340 ocurrencias\n",
      "  that's              :   332 ocurrencias\n",
      "  don't               :   324 ocurrencias\n",
      "\n",
      "TABLA COMPARATIVA DE COBERTURA\n",
      "                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\n",
      "Word2Vec (Google News)                      48.26                 67.48                  4295                     4604\n",
      " GloVe (Wiki Gigaword)                      47.57                 71.53                  4233                     4666\n",
      "  FastText (Wiki News)                      59.38                 77.92                  5284                     3615\n"
     ]
    }
   ],
   "source": [
    "# Obtener vocabulario del dataset\n",
    "all_tokens = []\n",
    "for text in df['dialog_clean']:\n",
    "    if pd.notna(text):\n",
    "        all_tokens.extend(text.split())\n",
    "\n",
    "vocab = set(all_tokens)\n",
    "vocab_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"\\nVocabulario total: {len(vocab):,} palabras únicas\")\n",
    "print(f\"Total de tokens: {len(all_tokens):,}\")\n",
    "\n",
    "# Análisis para cada modelo\n",
    "models = {\n",
    "    'Word2Vec (Google News)': w2v,\n",
    "    'GloVe (Wiki Gigaword)': glove,\n",
    "    'FastText (Wiki News)': ft\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    \n",
    "    found_words = [w for w in vocab if w in model.key_to_index]\n",
    "    missing_words = [w for w in vocab if w not in model.key_to_index]\n",
    "    \n",
    "    found_tokens = sum(vocab_freq[w] for w in found_words)\n",
    "    total_tokens = sum(vocab_freq.values())\n",
    "    \n",
    "    coverage_vocab = len(found_words) / len(vocab) * 100\n",
    "    coverage_tokens = found_tokens / total_tokens * 100\n",
    "    \n",
    "    print(f\"Palabras encontradas: {len(found_words):,}/{len(vocab):,} ({coverage_vocab:.2f}%)\")\n",
    "    print(f\"Tokens cubiertos: {found_tokens:,}/{total_tokens:,} ({coverage_tokens:.2f}%)\")\n",
    "    \n",
    "    missing_freq = {w: vocab_freq[w] for w in missing_words}\n",
    "    top_missing = sorted(missing_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\nTop 10 palabras NO encontradas (más frecuentes):\")\n",
    "    for word, freq in top_missing:\n",
    "        print(f\"  {word:20s}: {freq:5d} ocurrencias\")\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Cobertura Vocabulario (%)': round(coverage_vocab, 2),\n",
    "        'Cobertura Tokens (%)': round(coverage_tokens, 2),\n",
    "        'Palabras Encontradas': len(found_words),\n",
    "        'Palabras No Encontradas': len(missing_words)\n",
    "    })\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\nTABLA COMPARATIVA DE COBERTURA\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_nlp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
