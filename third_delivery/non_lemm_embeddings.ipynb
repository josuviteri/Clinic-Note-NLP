{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d05b10",
   "metadata": {},
   "source": [
    "# 1. Corregir las sugerencias que recibimos de la entrega 2:\n",
    "\n",
    "    No lematizar\n",
    "\n",
    "Para esto simplemente omitimos el paso de lematización.\n",
    "\n",
    "    Tokenización\n",
    "\n",
    "Este paso lo vamos a pulir, si bien es cierto que al final no estaba mal, vistos los resultados como la cobertura de los modelos de embeddings, se puede mejorar con unos cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473afd",
   "metadata": {},
   "source": [
    "Estos eran los resultados SIN limpieza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed619e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nTotal de palabras/tokens en todas las conversaciones ORIGINALMENTE: 126935\\nTotal de tokens: 126,935\\n\\nVocabulario total: 9,496 palabras/tokens únic@s\\n\\nModelo: Word2Vec (Google News)\\nPalabras encontradas: 4,766/9,496 (50.19%)\\nTokens cubiertos: 86,784/126,935 (68.37%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  to                  :  2077 ocurrencias\\n  a                   :  1983 ocurrencias\\n  and                 :  1822 ocurrencias\\n  of                  :  1567 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  No,                 :   477 ocurrencias\\n  Yeah,               :   302 ocurrencias\\n\\nModelo: GloVe (Wiki Gigaword)\\nPalabras encontradas: 3,941/9,496 (41.50%)\\nTokens cubiertos: 75,535/126,935 (59.51%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  I                   :  4720 ocurrencias\\n  Do                  :   648 ocurrencias\\n  How                 :   581 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  What                :   507 ocurrencias\\n  No,                 :   477 ocurrencias\\n  No.                 :   443 ocurrencias\\n\\nModelo: FastText (Wiki News)\\nPalabras encontradas: 5,763/9,496 (60.69%)\\nTokens cubiertos: 98,890/126,935 (77.91%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  No,                 :   477 ocurrencias\\n  I'm                 :   441 ocurrencias\\n  don't               :   310 ocurrencias\\n  Yeah,               :   302 ocurrencias\\n  Well,               :   256 ocurrencias\\n  it's                :   249 ocurrencias\\n\\nTABLA COMPARATIVA DE COBERTURA\\n                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\\nWord2Vec (Google News)                      50.19                 68.37                  4766                     4730\\n GloVe (Wiki Gigaword)                      41.50                 59.51                  3941                     5555\\n  FastText (Wiki News)                      60.69                 77.91                  5763                     3733 \""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Total de palabras/tokens en todas las conversaciones ORIGINALMENTE: 126935\n",
    "Total de tokens: 126,935\n",
    "\n",
    "Vocabulario total: 9,496 palabras/tokens únic@s\n",
    "\n",
    "Modelo: Word2Vec (Google News)\n",
    "Palabras encontradas: 4,766/9,496 (50.19%)\n",
    "Tokens cubiertos: 86,784/126,935 (68.37%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  to                  :  2077 ocurrencias\n",
    "  a                   :  1983 ocurrencias\n",
    "  and                 :  1822 ocurrencias\n",
    "  of                  :  1567 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  Yeah,               :   302 ocurrencias\n",
    "\n",
    "Modelo: GloVe (Wiki Gigaword)\n",
    "Palabras encontradas: 3,941/9,496 (41.50%)\n",
    "Tokens cubiertos: 75,535/126,935 (59.51%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  I                   :  4720 ocurrencias\n",
    "  Do                  :   648 ocurrencias\n",
    "  How                 :   581 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  What                :   507 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  No.                 :   443 ocurrencias\n",
    "\n",
    "Modelo: FastText (Wiki News)\n",
    "Palabras encontradas: 5,763/9,496 (60.69%)\n",
    "Tokens cubiertos: 98,890/126,935 (77.91%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  I'm                 :   441 ocurrencias\n",
    "  don't               :   310 ocurrencias\n",
    "  Yeah,               :   302 ocurrencias\n",
    "  Well,               :   256 ocurrencias\n",
    "  it's                :   249 ocurrencias\n",
    "\n",
    "TABLA COMPARATIVA DE COBERTURA\n",
    "                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\n",
    "Word2Vec (Google News)                      50.19                 68.37                  4766                     4730\n",
    " GloVe (Wiki Gigaword)                      41.50                 59.51                  3941                     5555\n",
    "  FastText (Wiki News)                      60.69                 77.91                  5763                     3733 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95205d9",
   "metadata": {},
   "source": [
    "Primero, todos los modelos tienen problemas con las palabras terminando en comas, y palabras abreviadas, por lo que las procesamos:\n",
    "\n",
    "  yes, -> yes\n",
    "  \n",
    "  i'm -> i am\n",
    "\n",
    "Además, aunque sí haciamos un  Doctor → DOC o Patient → PAT  , el marcador especial cambiaba a minuscula al final, lo arreglaremos.\n",
    "También hemos encontrado otros tags como Doctor_2, guest_family y guest_clinician. Los trataremos tambien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5908c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de librerías\n",
    "%pip install spacy gensim transformers torch tensorflow tensorflow-hub seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462f5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\estib\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\estib\\miniconda3\\envs\\nlp\\lib\\site-packages\\tensorflow_hub\\__init__.py:61: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\estib\\miniconda3\\envs\\nlp\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP y embeddings\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f527bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/MTS-Dialog-TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cd12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"here's\": \"here is\"\n",
    "}\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = text.lower()\n",
    "    for c, repl in contraction_map.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(c) + r\"\\b\", repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18241918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s, lowercase=True):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "\n",
    "    # Marcadores de quién habla -> usar tokens temporales\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' __doc__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bDoctor_2[:\\-]\\s*', ' __doc2__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' __pat__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family[:\\-]\\s*', ' __fam__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family_1[:\\-]\\s*', ' __fam__ ', s, flags=re.I) \n",
    "    #Si hay dos visitantes, el primero cambia de guest_family a guest_family_1, vamos a igualarlos, esté solo o no siempre será <FAMILY>\n",
    "    s = re.sub(r'\\bGuest_family_2[:\\-]\\s*', ' __fam2__ ', s, flags=re.I) \n",
    "    s = re.sub(r'\\bGuest_clinician[:\\-]\\s*', ' __clin__ ', s, flags=re.I)\n",
    "    \n",
    "    # Expand contractions (suponiendo que tienes esta función)\n",
    "    s = expand_contractions(s)\n",
    "\n",
    "    # Separar puntuación\n",
    "    s = re.sub(r'([.,!?;:()\"\\[\\]])', r' \\1 ', s)\n",
    "\n",
    "    # Reducir espacios\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    # Lowercase todo excepto los tags\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "\n",
    "    # Restaurar los tags en mayúsculas\n",
    "    s = s.replace('__doc__', '<DOC>')\n",
    "    s = s.replace('__doc2__', '<DOC2>')\n",
    "    s = s.replace('__pat__', '<PAT>')\n",
    "    s = s.replace('__fam__', '<FAMILY>')\n",
    "    s = s.replace('__fam2__', '<FAMILY2>')\n",
    "    s = s.replace('__clin__', '<CLIN>')\n",
    "\n",
    "    return s\n",
    " \n",
    "\n",
    "# Versión para ELMo (lowercase)\n",
    "df['dialog_clean'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=True))\n",
    "\n",
    "# Versión para BIO/ClinicalBERT (manteniendo mayúsculas)\n",
    "df['dialog_clean_clinicBERT'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=False))\n",
    "\n",
    "# Los resúmenes\n",
    "df['section_text_clean'] = df['section_text'].apply(lambda x: normalize_text(x, lowercase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed33552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dialog_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9886184a-855d-44ed-9a0b-7d8f1444a2fd",
       "rows": [
        [
         "0",
         "<DOC> what brings you back into the clinic today , miss ? <PAT> i came in for a refill of my blood pressure medicine . <DOC> it looks like doctor kumar followed up with you last time regarding your hypertension , osteoarthritis , osteoporosis , hypothyroidism , allergic rhinitis and kidney stones . have you noticed any changes or do you have any concerns regarding these issues ? <PAT> no . <DOC> have you had any fever or chills , cough , congestion , nausea , vomiting , chest pain , chest pressure ? <PAT> no . <DOC> great . also , for our records , how old are you and what race do you identify yourself as ? <PAT> i am seventy six years old and identify as a white female ."
        ],
        [
         "1",
         "<DOC> how are you feeling today ? <PAT> terrible . i am having the worst headache of my life . <DOC> i am so sorry . well you are only twenty five , so let us hope this is the last of the worst . let us see how we can best help you . when did it start ? <PAT> around eleven in the morning . <DOC> today ? <PAT> um no yesterday . july thirty first . <DOC> july thirty first o eight . got it . did it come on suddenly ? <PAT> yeah . <DOC> are you having any symptoms with it , such as blurry vision , light sensitivity , dizziness , lightheadedness , or nausea ? <PAT> i am having blurry vision and lightheadedness . i also can not seem to write well . it looks so messy . i am naturally right handed but my writing looks like i am trying with my left . <DOC> how would you describe the lightheadedness ? <PAT> like there are blind spots . <DOC> okay . how about any vomiting ? <PAT> um no . i feel like my face is pretty swollen though . i do not know if it is related to the headache but it started around the same time . <DOC> here in the e r , we will do a thorough exam and eval to make sure nothing serious is going on . while we are waiting for your c t results , i am going to order a migraine cocktail and some morphine . <PAT> thank . will the nurse be in soon ? <DOC> yes , she'll be right in as soon as the order is placed . it shouldn't be more than a few minutes . if it takes longer , then please ring the call bell ."
        ],
        [
         "2",
         "<DOC> hello , miss . what is the reason for your visit today ? <PAT> i think i have some warts on my back end where the poop comes out . <DOC> i see . when did you start noticing them ? <PAT> i think like three to four weeks ago . <DOC> do you feel any pain or discomfort ? <PAT> it itches a little , but i have not felt any pain yet . is this normal for a twenty two year old ? <DOC> i will have to take a look , but you will be fine . are there any other symptoms that you are aware of ? <PAT> nope . just the warts and itchiness ."
        ],
        [
         "3",
         "<DOC> are you taking any over the counter medicines ? <PAT> no , only the ones which were prescribed . <DOC> no alternative medicine , naturopathy or anything ? <PAT> no , only whatever is here in this prescription . <DOC> okay let me take a look . . . so you were prescribed salmeterol inhaler- <PAT> on as needed basis . <DOC> okay and the other one is fluticasone inhaler , which is- <PAT> which is a nasal inhaler . <DOC> right ."
        ],
        [
         "4",
         "<DOC> hi , how are you ? <PAT> i burned my hand . <DOC> oh , i am sorry . wow ! <PAT> yeah . <DOC> is it only right arm ? <PAT> yes ."
        ],
        [
         "5",
         "<DOC> how is your asthma since you started using your inhaler again ? <PAT> much better . i do not know why i did not take it with me everywhere i went . <DOC> it is important to carry it with you , especially during times where you are exercising or walking more than usual . <PAT> yeah . i think i have learned my lesson . <DOC> besides asthma , do you have any other medical problems ?"
        ],
        [
         "6",
         "<DOC> do you smoke ? <PAT> no , i quit before i had my daughter . <DOC> are you currently pregnant ? <PAT> no , i am not . <DOC> did you have any complications with the birth of your daughter ? <PAT> i actually had a c section . <DOC> have you had any other surgeries in the past ? <PAT> i got my appendix out a few years ago . <DOC> do you have any other issues , like high blood pressure or heart disease ? <PAT> no . <DOC> do you have diabetes ? <PAT> no . <DOC> are there any problems with the lungs , thyroid , kidney , or bladder ? <PAT> no . <DOC> so , how long ago did you hurt your lower back ? <PAT> it was about four or five years ago now , when i was in a car crash . <DOC> what kind of treatments were recommended ? <PAT> they did not recommend p t , and i did not really have any increased back pain after the accident ."
        ],
        [
         "7",
         "<DOC> any know drug allergies ? <PAT> no ."
        ],
        [
         "8",
         "<DOC> hi there , sir ! how are you today ? <PAT> hello ! i am good . <DOC> i would like to start with your family medical history today . what do you know about their medical history ? <PAT> my mother and father both had heart disease . well , my mother had complication from her heart disease and that is how she passed . my father was only in his forty's when he died . <DOC> i am so sorry the hear that . <PAT> thank you . i have two brothers , one whom i do not speak to very much anymore . i do not know if he has any health problems . my other brother is healthy with no issues . both my uncles on my mother's side had polio , i think . <DOC> tell me more about your uncles with polio . they both had polio ? <PAT> one of them had to wear crutches due to how bad his leg deformans were and then the other had leg deformities in only one leg . i am fairly certain that they had polio . <DOC> do you know of any other family member with neurological conditions ? <PAT> no . none that i know of . <DOC> do you have any children ? <PAT> yes . i have one child . <DOC> is your child healthy and well ? <PAT> yes ."
        ],
        [
         "9",
         "<DOC> can you tell me about any diseases that run in your family ? <PAT> sure , my brother has a prostate cancer . <DOC> okay , brother . <PAT> my father had brain cancer . <DOC> okay , dad . <PAT> then on both sides of my family there are many heart related issues . <DOC> okay . <PAT> and my brother and sister both have diabetes . <DOC> okay . <PAT> yes , that is it ."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "0    <DOC> what brings you back into the clinic tod...\n",
       "1    <DOC> how are you feeling today ? <PAT> terrib...\n",
       "2    <DOC> hello , miss . what is the reason for yo...\n",
       "3    <DOC> are you taking any over the counter medi...\n",
       "4    <DOC> hi , how are you ? <PAT> i burned my han...\n",
       "5    <DOC> how is your asthma since you started usi...\n",
       "6    <DOC> do you smoke ? <PAT> no , i quit before ...\n",
       "7           <DOC> any know drug allergies ? <PAT> no .\n",
       "8    <DOC> hi there , sir ! how are you today ? <PA...\n",
       "9    <DOC> can you tell me about any diseases that ...\n",
       "Name: dialog_clean, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dialog_clean'].head(10)  # primeras 10 filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc32b13",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Embeddings (sin lemmatization)\n",
    "w2v = api.load(\"word2vec-google-news-300\")          # Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e6039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-300\")        # GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ca6d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = api.load(\"fasttext-wiki-news-subwords-300\")   # FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cb8956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras/tokens: 152,151\n",
      "\n",
      "Vocabulario total: 5,349 palabras/tokens únic@s\n",
      "\n",
      "Modelo: Word2Vec (Google News)\n",
      "Palabras encontradas: 5,069/5,349 (94.77%)\n",
      "Tokens cubiertos: 109,869/152,151 (72.21%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  .                   : 10594 ocurrencias\n",
      "  ,                   :  6596 ocurrencias\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  ?                   :  5113 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  a                   :  2121 ocurrencias\n",
      "  to                  :  2104 ocurrencias\n",
      "  and                 :  2033 ocurrencias\n",
      "  of                  :  1654 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "\n",
      "Modelo: GloVe (Wiki Gigaword)\n",
      "Palabras encontradas: 5,057/5,349 (94.54%)\n",
      "Tokens cubiertos: 140,287/152,151 (92.20%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "  <CLIN>              :   119 ocurrencias\n",
      "  <DOC2>              :    43 ocurrencias\n",
      "  antiinflammatories  :    11 ocurrencias\n",
      "  dad's               :    10 ocurrencias\n",
      "  <FAMILY2>           :     9 ocurrencias\n",
      "  mom's               :     7 ocurrencias\n",
      "  cetaphil            :     7 ocurrencias\n",
      "\n",
      "Modelo: FastText (Wiki News)\n",
      "Palabras encontradas: 5,085/5,349 (95.06%)\n",
      "Tokens cubiertos: 140,286/152,151 (92.20%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "  <CLIN>              :   119 ocurrencias\n",
      "  <DOC2>              :    43 ocurrencias\n",
      "  antiinflammatories  :    11 ocurrencias\n",
      "  dad's               :    10 ocurrencias\n",
      "  <FAMILY2>           :     9 ocurrencias\n",
      "  lexapro             :     7 ocurrencias\n",
      "  mom's               :     7 ocurrencias\n",
      "\n",
      "TABLA COMPARATIVA DE COBERTURA\n",
      "                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\n",
      "Word2Vec (Google News)                      94.77                 72.21                  5069                      280\n",
      " GloVe (Wiki Gigaword)                      94.54                 92.20                  5057                      292\n",
      "  FastText (Wiki News)                      95.06                 92.20                  5085                      264\n"
     ]
    }
   ],
   "source": [
    "# Obtener vocabulario del dataset\n",
    "all_tokens = []\n",
    "for text in df['dialog_clean']:\n",
    "    if pd.notna(text):\n",
    "        all_tokens.extend(text.split())\n",
    "\n",
    "vocab = set(all_tokens)\n",
    "vocab_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"Total de palabras/tokens: {len(all_tokens):,}\")\n",
    "print(f\"\\nVocabulario total: {len(vocab):,} palabras/tokens únic@s\")\n",
    "\n",
    "\n",
    "# Análisis para cada modelo\n",
    "models = {\n",
    "    'Word2Vec (Google News)': w2v,\n",
    "    'GloVe (Wiki Gigaword)': glove,\n",
    "    'FastText (Wiki News)': ft\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    \n",
    "    found_words = [w for w in vocab if w in model.key_to_index]\n",
    "    missing_words = [w for w in vocab if w not in model.key_to_index]\n",
    "    \n",
    "    found_tokens = sum(vocab_freq[w] for w in found_words)\n",
    "    total_tokens = sum(vocab_freq.values())\n",
    "    \n",
    "    coverage_vocab = len(found_words) / len(vocab) * 100\n",
    "    coverage_tokens = found_tokens / total_tokens * 100\n",
    "    \n",
    "    print(f\"Palabras encontradas: {len(found_words):,}/{len(vocab):,} ({coverage_vocab:.2f}%)\")\n",
    "    print(f\"Tokens cubiertos: {found_tokens:,}/{total_tokens:,} ({coverage_tokens:.2f}%)\")\n",
    "    \n",
    "    missing_freq = {w: vocab_freq[w] for w in missing_words}\n",
    "    top_missing = sorted(missing_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\nTop 10 palabras NO encontradas (más frecuentes):\")\n",
    "    for word, freq in top_missing:\n",
    "        print(f\"  {word:20s}: {freq:5d} ocurrencias\")\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Cobertura Vocabulario (%)': round(coverage_vocab, 2),\n",
    "        'Cobertura Tokens (%)': round(coverage_tokens, 2),\n",
    "        'Palabras Encontradas': len(found_words),\n",
    "        'Palabras No Encontradas': len(missing_words)\n",
    "    })\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\nTABLA COMPARATIVA DE COBERTURA\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b807607",
   "metadata": {},
   "source": [
    "Tras estos cambios, se generan 26,000 tokens mas al separar los signos de puntuacion. A cambio, el % de cobertura pasa de 68-69-77% a 72-92-92% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34499032",
   "metadata": {},
   "source": [
    "### 6. Embeddings Contextuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaccc48",
   "metadata": {},
   "source": [
    "    Dos métodos: ELMo y BERT. Para clínico, variantes de Bio/Clinical (BioBERT + ClinicalBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f991b32",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo ELMo pre-entrenado desde TensorFlow Hub\n",
    "print(\"\\nCargando modelo ELMo desde TensorFlow Hub...\")\n",
    "elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(\"Modelo ELMo cargado (dimensión: 1024)\")\n",
    "\n",
    "def get_elmo_embeddings(texts, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de ELMo para una lista de textos completos.\n",
    "    ELMo produce embeddings de 1024 dimensiones.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcesando {len(texts)} textos con ELMo...\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Convertir a tensores de TensorFlow\n",
    "        embeddings = elmo_model.signatures[\"default\"](\n",
    "            tf.constant(batch_texts)\n",
    "        )[\"elmo\"]\n",
    "        \n",
    "        # Promedio de todos los tokens para obtener vector de secuencia\n",
    "        sentence_embeddings = tf.reduce_mean(embeddings, axis=1)\n",
    "        all_embeddings.extend(sentence_embeddings.numpy())\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Procesar todo el dataset\n",
    "all_texts = df['dialog_clean'].tolist()\n",
    "\n",
    "print(f\"\\nProcesando {len(all_texts)} diálogos (todo el dataset)...\")\n",
    "elmo_embeddings = get_elmo_embeddings(all_texts, batch_size=16)  # batch_size ajustable según RAM\n",
    "\n",
    "print(f\"\\nShape de embeddings: {elmo_embeddings.shape}\")\n",
    "print(f\"Dimensión por embedding: {elmo_embeddings.shape[1]}\")\n",
    "\n",
    "# Guardar embeddings\n",
    "embedding_data = {\n",
    "    'indices': list(range(len(all_texts))),\n",
    "    'embeddings': elmo_embeddings,\n",
    "    'shape': elmo_embeddings.shape,\n",
    "    'model_name': 'ELMo (TensorFlow Hub)',\n",
    "    'embedding_dim': elmo_embeddings.shape[1]\n",
    "}\n",
    "\n",
    "with open('processed/elmo_embeddings_full.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_data, f)\n",
    "\n",
    "print(f\"\\nEmbeddings ELMo guardados en: processed/elmo_embeddings_full.pkl\")\n",
    "print(f\"  Total de diálogos procesados: {len(all_texts)}\")\n",
    "print(f\"  Dimensión: {elmo_embeddings.shape[1]}\")\n",
    "\n",
    "# Añadir referencias al DataFrame\n",
    "df['has_elmo_embedding'] = True\n",
    "df['elmo_embedding_idx'] = list(range(len(all_texts)))\n",
    "\n",
    "print(\"\\nReferencias añadidas al DataFrame para todos los diálogos\")\n",
    "\n",
    "# Ejemplo de similitud para los primeros 5\n",
    "print(\"\\nEJEMPLO: Similitudes ELMo entre primeros 5 diálogos\")\n",
    "sim_matrix = cosine_similarity(elmo_embeddings[:5])\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        print(f\"Diálogo {i} vs Diálogo {j}: similitud = {sim_matrix[i,j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d37ecd",
   "metadata": {},
   "source": [
    "### BERT / BIO/ClinicalBERT (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff178d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, model_name, max_length=512, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de BERT para todos los textos.\n",
    "    Usa el token [CLS] como representación de la secuencia completa.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCargando modelo: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.extend(cls_embeddings)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Modelo BioClinicalBERT preentrenado en textos clínicos\n",
    "selected_model = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "\n",
    "# Procesar TODO el dataset\n",
    "all_texts_bert = df['dialog_clean_clinicBERT'].tolist()\n",
    "\n",
    "print(f\"\\nProcesando {len(all_texts_bert)} diálogos con BioClinicalBERT...\")\n",
    "bert_embeddings = get_bert_embeddings(all_texts_bert, selected_model, batch_size=8)\n",
    "\n",
    "print(f\"\\nShape de embeddings: {bert_embeddings.shape}\")\n",
    "print(f\"Dimensión por embedding: {bert_embeddings.shape[1]}\")\n",
    "\n",
    "# Guardar embeddings en pickle\n",
    "embedding_data = {\n",
    "    'indices': list(range(len(all_texts_bert))),\n",
    "    'embeddings': bert_embeddings,\n",
    "    'shape': bert_embeddings.shape,\n",
    "    'model_name': selected_model,\n",
    "    'embedding_dim': bert_embeddings.shape[1]\n",
    "}\n",
    "\n",
    "with open('processed/clinical_bert_embeddings_full.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_data, f)\n",
    "\n",
    "print(f\"\\nEmbeddings BioClinicalBERT guardados en: processed/clinical_bert_embeddings_full.pkl\")\n",
    "\n",
    "# Añadir referencias al DataFrame\n",
    "df['has_bert_embedding'] = True\n",
    "df['bert_embedding_idx'] = list(range(len(all_texts_bert)))\n",
    "\n",
    "print(\"\\nReferencias añadidas al DataFrame para todos los diálogos\")\n",
    "\n",
    "# Ejemplo de similitud para los primeros 5\n",
    "print(\"\\nEJEMPLO: Similitudes entre primeros 5 diálogos\")\n",
    "sim_matrix = cosine_similarity(bert_embeddings[:5])\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        print(f\"Diálogo {i} vs Diálogo {j}: similitud = {sim_matrix[i,j]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
