{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d05b10",
   "metadata": {},
   "source": [
    "# 1. Corregir las sugerencias que recibimos de la entrega 2:\n",
    "\n",
    "    No lematizar\n",
    "\n",
    "Para esto simplemente omitimos el paso de lematización.\n",
    "\n",
    "    Tokenización\n",
    "\n",
    "Este paso lo vamos a pulir, si bien es cierto que al final no estaba mal, vistos los resultados como la cobertura de los modelos de embeddings, se puede mejorar con unos cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4473afd",
   "metadata": {},
   "source": [
    "Estos eran los resultados previos CON lemmatización y la normalización previa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed619e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nTotal de palabras/tokens en todas las conversaciones ORIGINALMENTE: 126935\\nTotal de tokens: 126,935\\n\\nVocabulario total: 9,496 palabras/tokens únic@s\\n\\nModelo: Word2Vec (Google News)\\nPalabras encontradas: 4,766/9,496 (50.19%)\\nTokens cubiertos: 86,784/126,935 (68.37%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  to                  :  2077 ocurrencias\\n  a                   :  1983 ocurrencias\\n  and                 :  1822 ocurrencias\\n  of                  :  1567 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  No,                 :   477 ocurrencias\\n  Yeah,               :   302 ocurrencias\\n\\nModelo: GloVe (Wiki Gigaword)\\nPalabras encontradas: 3,941/9,496 (41.50%)\\nTokens cubiertos: 75,535/126,935 (59.51%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  I                   :  4720 ocurrencias\\n  Do                  :   648 ocurrencias\\n  How                 :   581 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  What                :   507 ocurrencias\\n  No,                 :   477 ocurrencias\\n  No.                 :   443 ocurrencias\\n\\nModelo: FastText (Wiki News)\\nPalabras encontradas: 5,763/9,496 (60.69%)\\nTokens cubiertos: 98,890/126,935 (77.91%)\\n\\nTop 10 palabras NO encontradas (más frecuentes):\\n  Doctor:             :  5809 ocurrencias\\n  Patient:            :  4895 ocurrencias\\n  Guest_family:       :   559 ocurrencias\\n  Yes,                :   518 ocurrencias\\n  No,                 :   477 ocurrencias\\n  I'm                 :   441 ocurrencias\\n  don't               :   310 ocurrencias\\n  Yeah,               :   302 ocurrencias\\n  Well,               :   256 ocurrencias\\n  it's                :   249 ocurrencias\\n\\nTABLA COMPARATIVA DE COBERTURA\\n                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\\nWord2Vec (Google News)                      50.19                 68.37                  4766                     4730\\n GloVe (Wiki Gigaword)                      41.50                 59.51                  3941                     5555\\n  FastText (Wiki News)                      60.69                 77.91                  5763                     3733 \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Total de palabras/tokens en todas las conversaciones ORIGINALMENTE: 126935\n",
    "Total de tokens: 126,935\n",
    "\n",
    "Vocabulario total: 9,496 palabras/tokens únic@s\n",
    "\n",
    "Modelo: Word2Vec (Google News)\n",
    "Palabras encontradas: 4,766/9,496 (50.19%)\n",
    "Tokens cubiertos: 86,784/126,935 (68.37%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  to                  :  2077 ocurrencias\n",
    "  a                   :  1983 ocurrencias\n",
    "  and                 :  1822 ocurrencias\n",
    "  of                  :  1567 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  Yeah,               :   302 ocurrencias\n",
    "\n",
    "Modelo: GloVe (Wiki Gigaword)\n",
    "Palabras encontradas: 3,941/9,496 (41.50%)\n",
    "Tokens cubiertos: 75,535/126,935 (59.51%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  I                   :  4720 ocurrencias\n",
    "  Do                  :   648 ocurrencias\n",
    "  How                 :   581 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  What                :   507 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  No.                 :   443 ocurrencias\n",
    "\n",
    "Modelo: FastText (Wiki News)\n",
    "Palabras encontradas: 5,763/9,496 (60.69%)\n",
    "Tokens cubiertos: 98,890/126,935 (77.91%)\n",
    "\n",
    "Top 10 palabras NO encontradas (más frecuentes):\n",
    "  Doctor:             :  5809 ocurrencias\n",
    "  Patient:            :  4895 ocurrencias\n",
    "  Guest_family:       :   559 ocurrencias\n",
    "  Yes,                :   518 ocurrencias\n",
    "  No,                 :   477 ocurrencias\n",
    "  I'm                 :   441 ocurrencias\n",
    "  don't               :   310 ocurrencias\n",
    "  Yeah,               :   302 ocurrencias\n",
    "  Well,               :   256 ocurrencias\n",
    "  it's                :   249 ocurrencias\n",
    "\n",
    "TABLA COMPARATIVA DE COBERTURA\n",
    "                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\n",
    "Word2Vec (Google News)                      50.19                 68.37                  4766                     4730\n",
    " GloVe (Wiki Gigaword)                      41.50                 59.51                  3941                     5555\n",
    "  FastText (Wiki News)                      60.69                 77.91                  5763                     3733 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95205d9",
   "metadata": {},
   "source": [
    "Primero, todos los modelos tienen problemas con las palabras terminando en comas, y palabras abreviadas, por lo que las procesamos:\n",
    "\n",
    "  yes, -> yes\n",
    "  \n",
    "  i'm -> i am\n",
    "\n",
    "Además, aunque sí haciamos un  Doctor → DOC o Patient → PAT  , el marcador especial cambiaba a minuscula al final, lo arreglaremos.\n",
    "También hemos encontrado otros tags como Doctor_2, guest_family y guest_clinician. Los trataremos tambien. Esperamos que estos cambios puedan mejorar el trabajo que hacen los 3 modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5908c849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (3.8.7)\n",
      "Requirement already satisfied: gensim in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (2.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
      "Collecting tensorflow-hub\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: seaborn in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (3.10.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (2.12.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (75.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (1.15.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: namex in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\iker\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Using cached tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: tf-keras, tensorflow-hub\n",
      "\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   ---------------------------------------- 0/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tensorflow-hub]\n",
      "   -------------------- ------------------- 1/2 [tensorflow-hub]\n",
      "   ---------------------------------------- 2/2 [tensorflow-hub]\n",
      "\n",
      "Successfully installed tensorflow-hub-0.16.1 tf-keras-2.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías\n",
    "%pip install spacy gensim transformers torch tensorflow tensorflow-hub seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462f5313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Iker\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Iker\\AppData\\Roaming\\Python\\Python313\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP y embeddings\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f527bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/MTS-Dialog-TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11cd12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"here's\": \"here is\"\n",
    "}\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = text.lower()\n",
    "    for c, repl in contraction_map.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(c) + r\"\\b\", repl, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18241918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s, lowercase=True):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "\n",
    "    # Marcadores de quién habla -> usar tokens temporales\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' __doc__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bDoctor_2[:\\-]\\s*', ' __doc2__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' __pat__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family[:\\-]\\s*', ' __fam__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family_1[:\\-]\\s*', ' __fam__ ', s, flags=re.I) \n",
    "    #Si hay dos visitantes, el primero cambia de guest_family a guest_family_1, vamos a igualarlos, esté solo o no siempre será <FAMILY>\n",
    "    s = re.sub(r'\\bGuest_family_2[:\\-]\\s*', ' __fam2__ ', s, flags=re.I) \n",
    "    s = re.sub(r'\\bGuest_clinician[:\\-]\\s*', ' __clin__ ', s, flags=re.I)\n",
    "    \n",
    "    # Expand contractions (suponiendo que tienes esta función)\n",
    "    s = expand_contractions(s)\n",
    "\n",
    "    # Separar puntuación\n",
    "    s = re.sub(r'([.,!?;:()\"\\[\\]])', r' \\1 ', s)\n",
    "\n",
    "    # Reducir espacios\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    # Lowercase todo excepto los tags\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "\n",
    "    # Restaurar los tags en mayúsculas\n",
    "    s = s.replace('__doc__', '<DOC>')\n",
    "    s = s.replace('__doc2__', '<DOC2>')\n",
    "    s = s.replace('__pat__', '<PAT>')\n",
    "    s = s.replace('__fam__', '<FAMILY>')\n",
    "    s = s.replace('__fam2__', '<FAMILY2>')\n",
    "    s = s.replace('__clin__', '<CLIN>')\n",
    "\n",
    "    return s\n",
    " \n",
    "\n",
    "# Versión para ELMo (lowercase)\n",
    "df['dialog_clean'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=True))\n",
    "\n",
    "# Versión para BIO/ClinicalBERT (manteniendo mayúsculas)\n",
    "df['dialog_clean_clinicBERT'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=False))\n",
    "\n",
    "# Los resúmenes\n",
    "df['section_text_clean'] = df['section_text'].apply(lambda x: normalize_text(x, lowercase=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed33552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dialog_clean",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "da6ff9ec-19ed-44eb-ba1c-6011f68364e6",
       "rows": [
        [
         "0",
         "<DOC> what brings you back into the clinic today , miss ? <PAT> i came in for a refill of my blood pressure medicine . <DOC> it looks like doctor kumar followed up with you last time regarding your hypertension , osteoarthritis , osteoporosis , hypothyroidism , allergic rhinitis and kidney stones . have you noticed any changes or do you have any concerns regarding these issues ? <PAT> no . <DOC> have you had any fever or chills , cough , congestion , nausea , vomiting , chest pain , chest pressure ? <PAT> no . <DOC> great . also , for our records , how old are you and what race do you identify yourself as ? <PAT> i am seventy six years old and identify as a white female ."
        ],
        [
         "1",
         "<DOC> how are you feeling today ? <PAT> terrible . i am having the worst headache of my life . <DOC> i am so sorry . well you are only twenty five , so let us hope this is the last of the worst . let us see how we can best help you . when did it start ? <PAT> around eleven in the morning . <DOC> today ? <PAT> um no yesterday . july thirty first . <DOC> july thirty first o eight . got it . did it come on suddenly ? <PAT> yeah . <DOC> are you having any symptoms with it , such as blurry vision , light sensitivity , dizziness , lightheadedness , or nausea ? <PAT> i am having blurry vision and lightheadedness . i also can not seem to write well . it looks so messy . i am naturally right handed but my writing looks like i am trying with my left . <DOC> how would you describe the lightheadedness ? <PAT> like there are blind spots . <DOC> okay . how about any vomiting ? <PAT> um no . i feel like my face is pretty swollen though . i do not know if it is related to the headache but it started around the same time . <DOC> here in the e r , we will do a thorough exam and eval to make sure nothing serious is going on . while we are waiting for your c t results , i am going to order a migraine cocktail and some morphine . <PAT> thank . will the nurse be in soon ? <DOC> yes , she'll be right in as soon as the order is placed . it shouldn't be more than a few minutes . if it takes longer , then please ring the call bell ."
        ],
        [
         "2",
         "<DOC> hello , miss . what is the reason for your visit today ? <PAT> i think i have some warts on my back end where the poop comes out . <DOC> i see . when did you start noticing them ? <PAT> i think like three to four weeks ago . <DOC> do you feel any pain or discomfort ? <PAT> it itches a little , but i have not felt any pain yet . is this normal for a twenty two year old ? <DOC> i will have to take a look , but you will be fine . are there any other symptoms that you are aware of ? <PAT> nope . just the warts and itchiness ."
        ],
        [
         "3",
         "<DOC> are you taking any over the counter medicines ? <PAT> no , only the ones which were prescribed . <DOC> no alternative medicine , naturopathy or anything ? <PAT> no , only whatever is here in this prescription . <DOC> okay let me take a look . . . so you were prescribed salmeterol inhaler- <PAT> on as needed basis . <DOC> okay and the other one is fluticasone inhaler , which is- <PAT> which is a nasal inhaler . <DOC> right ."
        ],
        [
         "4",
         "<DOC> hi , how are you ? <PAT> i burned my hand . <DOC> oh , i am sorry . wow ! <PAT> yeah . <DOC> is it only right arm ? <PAT> yes ."
        ],
        [
         "5",
         "<DOC> how is your asthma since you started using your inhaler again ? <PAT> much better . i do not know why i did not take it with me everywhere i went . <DOC> it is important to carry it with you , especially during times where you are exercising or walking more than usual . <PAT> yeah . i think i have learned my lesson . <DOC> besides asthma , do you have any other medical problems ?"
        ],
        [
         "6",
         "<DOC> do you smoke ? <PAT> no , i quit before i had my daughter . <DOC> are you currently pregnant ? <PAT> no , i am not . <DOC> did you have any complications with the birth of your daughter ? <PAT> i actually had a c section . <DOC> have you had any other surgeries in the past ? <PAT> i got my appendix out a few years ago . <DOC> do you have any other issues , like high blood pressure or heart disease ? <PAT> no . <DOC> do you have diabetes ? <PAT> no . <DOC> are there any problems with the lungs , thyroid , kidney , or bladder ? <PAT> no . <DOC> so , how long ago did you hurt your lower back ? <PAT> it was about four or five years ago now , when i was in a car crash . <DOC> what kind of treatments were recommended ? <PAT> they did not recommend p t , and i did not really have any increased back pain after the accident ."
        ],
        [
         "7",
         "<DOC> any know drug allergies ? <PAT> no ."
        ],
        [
         "8",
         "<DOC> hi there , sir ! how are you today ? <PAT> hello ! i am good . <DOC> i would like to start with your family medical history today . what do you know about their medical history ? <PAT> my mother and father both had heart disease . well , my mother had complication from her heart disease and that is how she passed . my father was only in his forty's when he died . <DOC> i am so sorry the hear that . <PAT> thank you . i have two brothers , one whom i do not speak to very much anymore . i do not know if he has any health problems . my other brother is healthy with no issues . both my uncles on my mother's side had polio , i think . <DOC> tell me more about your uncles with polio . they both had polio ? <PAT> one of them had to wear crutches due to how bad his leg deformans were and then the other had leg deformities in only one leg . i am fairly certain that they had polio . <DOC> do you know of any other family member with neurological conditions ? <PAT> no . none that i know of . <DOC> do you have any children ? <PAT> yes . i have one child . <DOC> is your child healthy and well ? <PAT> yes ."
        ],
        [
         "9",
         "<DOC> can you tell me about any diseases that run in your family ? <PAT> sure , my brother has a prostate cancer . <DOC> okay , brother . <PAT> my father had brain cancer . <DOC> okay , dad . <PAT> then on both sides of my family there are many heart related issues . <DOC> okay . <PAT> and my brother and sister both have diabetes . <DOC> okay . <PAT> yes , that is it ."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 10
       }
      },
      "text/plain": [
       "0    <DOC> what brings you back into the clinic tod...\n",
       "1    <DOC> how are you feeling today ? <PAT> terrib...\n",
       "2    <DOC> hello , miss . what is the reason for yo...\n",
       "3    <DOC> are you taking any over the counter medi...\n",
       "4    <DOC> hi , how are you ? <PAT> i burned my han...\n",
       "5    <DOC> how is your asthma since you started usi...\n",
       "6    <DOC> do you smoke ? <PAT> no , i quit before ...\n",
       "7           <DOC> any know drug allergies ? <PAT> no .\n",
       "8    <DOC> hi there , sir ! how are you today ? <PA...\n",
       "9    <DOC> can you tell me about any diseases that ...\n",
       "Name: dialog_clean, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dialog_clean'].head(10)  # primeras 10 filas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bc32b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings (sin lemmatization)\n",
    "w2v = api.load(\"word2vec-google-news-300\")          # Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41e6039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load(\"glove-wiki-gigaword-300\")        # GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca6d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = api.load(\"fasttext-wiki-news-subwords-300\")   # FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb8956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras/tokens: 152,151\n",
      "\n",
      "Vocabulario total: 5,349 palabras/tokens únic@s\n",
      "\n",
      "Modelo: Word2Vec (Google News)\n",
      "Palabras encontradas: 5,069/5,349 (94.77%)\n",
      "Tokens cubiertos: 109,869/152,151 (72.21%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  .                   : 10594 ocurrencias\n",
      "  ,                   :  6596 ocurrencias\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  ?                   :  5113 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  a                   :  2121 ocurrencias\n",
      "  to                  :  2104 ocurrencias\n",
      "  and                 :  2033 ocurrencias\n",
      "  of                  :  1654 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "\n",
      "Modelo: GloVe (Wiki Gigaword)\n",
      "Palabras encontradas: 5,057/5,349 (94.54%)\n",
      "Tokens cubiertos: 140,287/152,151 (92.20%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "  <CLIN>              :   119 ocurrencias\n",
      "  <DOC2>              :    43 ocurrencias\n",
      "  antiinflammatories  :    11 ocurrencias\n",
      "  dad's               :    10 ocurrencias\n",
      "  <FAMILY2>           :     9 ocurrencias\n",
      "  mom's               :     7 ocurrencias\n",
      "  cetaphil            :     7 ocurrencias\n",
      "\n",
      "Modelo: FastText (Wiki News)\n",
      "Palabras encontradas: 5,085/5,349 (95.06%)\n",
      "Tokens cubiertos: 140,286/152,151 (92.20%)\n",
      "\n",
      "Top 10 palabras NO encontradas (más frecuentes):\n",
      "  <DOC>               :  5810 ocurrencias\n",
      "  <PAT>               :  4895 ocurrencias\n",
      "  <FAMILY>            :   575 ocurrencias\n",
      "  <CLIN>              :   119 ocurrencias\n",
      "  <DOC2>              :    43 ocurrencias\n",
      "  antiinflammatories  :    11 ocurrencias\n",
      "  dad's               :    10 ocurrencias\n",
      "  <FAMILY2>           :     9 ocurrencias\n",
      "  lexapro             :     7 ocurrencias\n",
      "  mom's               :     7 ocurrencias\n",
      "\n",
      "TABLA COMPARATIVA DE COBERTURA\n",
      "                Modelo  Cobertura Vocabulario (%)  Cobertura Tokens (%)  Palabras Encontradas  Palabras No Encontradas\n",
      "Word2Vec (Google News)                      94.77                 72.21                  5069                      280\n",
      " GloVe (Wiki Gigaword)                      94.54                 92.20                  5057                      292\n",
      "  FastText (Wiki News)                      95.06                 92.20                  5085                      264\n"
     ]
    }
   ],
   "source": [
    "# Obtener vocabulario del dataset\n",
    "all_tokens = []\n",
    "for text in df['dialog_clean']:\n",
    "    if pd.notna(text):\n",
    "        all_tokens.extend(text.split())\n",
    "\n",
    "vocab = set(all_tokens)\n",
    "vocab_freq = Counter(all_tokens)\n",
    "\n",
    "print(f\"Total de palabras/tokens: {len(all_tokens):,}\")\n",
    "print(f\"\\nVocabulario total: {len(vocab):,} palabras/tokens únic@s\")\n",
    "\n",
    "\n",
    "# Análisis para cada modelo\n",
    "models = {\n",
    "    'Word2Vec (Google News)': w2v,\n",
    "    'GloVe (Wiki Gigaword)': glove,\n",
    "    'FastText (Wiki News)': ft\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nModelo: {model_name}\")\n",
    "    \n",
    "    found_words = [w for w in vocab if w in model.key_to_index]\n",
    "    missing_words = [w for w in vocab if w not in model.key_to_index]\n",
    "    \n",
    "    found_tokens = sum(vocab_freq[w] for w in found_words)\n",
    "    total_tokens = sum(vocab_freq.values())\n",
    "    \n",
    "    coverage_vocab = len(found_words) / len(vocab) * 100\n",
    "    coverage_tokens = found_tokens / total_tokens * 100\n",
    "    \n",
    "    print(f\"Palabras encontradas: {len(found_words):,}/{len(vocab):,} ({coverage_vocab:.2f}%)\")\n",
    "    print(f\"Tokens cubiertos: {found_tokens:,}/{total_tokens:,} ({coverage_tokens:.2f}%)\")\n",
    "    \n",
    "    missing_freq = {w: vocab_freq[w] for w in missing_words}\n",
    "    top_missing = sorted(missing_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"\\nTop 10 palabras NO encontradas (más frecuentes):\")\n",
    "    for word, freq in top_missing:\n",
    "        print(f\"  {word:20s}: {freq:5d} ocurrencias\")\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': model_name,\n",
    "        'Cobertura Vocabulario (%)': round(coverage_vocab, 2),\n",
    "        'Cobertura Tokens (%)': round(coverage_tokens, 2),\n",
    "        'Palabras Encontradas': len(found_words),\n",
    "        'Palabras No Encontradas': len(missing_words)\n",
    "    })\n",
    "\n",
    "# Tabla comparativa\n",
    "print(\"\\nTABLA COMPARATIVA DE COBERTURA\")\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b807607",
   "metadata": {},
   "source": [
    "Tras estos cambios, se generan 26,000 tokens más al separar los signos de puntuacion. A cambio, el % de cobertura pasa de 68-69-77% a 72-92-92%. \n",
    "\n",
    "Los cambios no han mejorado el Word2Vec tanto como esperabamos, pero por minima que sea, si ha mejrado es que algo hemos hecho bien. Afortunadamente, quitar la lematización y mejorar la normalización han permitido un aumento muy importante en GloVe y FastText. Si nos fijamos, las palabras que no han encontrado son los TAGS, palabras extrañas como lexapro o palabras compuestas como dad's y mom's que no podemos descomponer ya que tienen varios significados (\"dad's\" puede ser \"dad is\" o el posesivo de dad). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34499032",
   "metadata": {},
   "source": [
    "### Embeddings Contextuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780baba3",
   "metadata": {},
   "source": [
    "En principio solo normalizabamos para los embeddings contextuales y no lematizabamos. Pero ya que también hemos hecho cambios en la normalización, estos cambios les competen y volveremos a correr su código para tener los embeddings actualizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaccc48",
   "metadata": {},
   "source": [
    "    Dos métodos: ELMo y BERT. Para clínico, variantes de Bio/Clinical (BioBERT + ClinicalBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f991b32",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6ae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando modelo ELMo desde TensorFlow Hub...\n",
      "Modelo ELMo cargado (dimensión: 1024)\n",
      "\n",
      "Procesando 1201 diálogos (todo el dataset)...\n",
      "\n",
      "Procesando 1201 textos con ELMo...\n",
      "  Procesados 160/1201\n",
      "  Procesados 320/1201\n",
      "  Procesados 480/1201\n",
      "  Procesados 640/1201\n",
      "  Procesados 800/1201\n",
      "  Procesados 960/1201\n",
      "  Procesados 1120/1201\n",
      "\n",
      "Shape de embeddings: (1201, 1024)\n",
      "Dimensión por embedding: 1024\n",
      "\n",
      "Embeddings ELMo guardados en: processed/elmo_embeddings_full.pkl y elmo_embeddings_tsv\n",
      "  Total de diálogos procesados: 1201\n",
      "  Dimensión: 1024\n",
      "\n",
      "Referencias añadidas al DataFrame para todos los diálogos\n",
      "\n",
      "EJEMPLO: Similitudes ELMo entre primeros 5 diálogos\n",
      "Diálogo 0 vs Diálogo 1: similitud = 0.8343\n",
      "Diálogo 0 vs Diálogo 2: similitud = 0.9520\n",
      "Diálogo 0 vs Diálogo 3: similitud = 0.9009\n",
      "Diálogo 0 vs Diálogo 4: similitud = 0.8088\n",
      "Diálogo 1 vs Diálogo 2: similitud = 0.8483\n",
      "Diálogo 1 vs Diálogo 3: similitud = 0.6911\n",
      "Diálogo 1 vs Diálogo 4: similitud = 0.5161\n",
      "Diálogo 2 vs Diálogo 3: similitud = 0.9122\n",
      "Diálogo 2 vs Diálogo 4: similitud = 0.8534\n",
      "Diálogo 3 vs Diálogo 4: similitud = 0.9091\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo ELMo pre-entrenado desde TensorFlow Hub\n",
    "print(\"\\nCargando modelo ELMo desde TensorFlow Hub...\")\n",
    "elmo_model = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
    "print(\"Modelo ELMo cargado (dimensión: 1024)\")\n",
    "\n",
    "def get_elmo_embeddings(texts, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de ELMo para una lista de textos completos.\n",
    "    ELMo produce embeddings de 1024 dimensiones.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcesando {len(texts)} textos con ELMo...\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        # Convertir a tensores de TensorFlow\n",
    "        embeddings = elmo_model.signatures[\"default\"](\n",
    "            tf.constant(batch_texts)\n",
    "        )[\"elmo\"]\n",
    "        \n",
    "        # Promedio de todos los tokens para obtener vector de secuencia\n",
    "        sentence_embeddings = tf.reduce_mean(embeddings, axis=1)\n",
    "        all_embeddings.extend(sentence_embeddings.numpy())\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Procesar todo el dataset\n",
    "all_texts = df['dialog_clean'].tolist()\n",
    "\n",
    "print(f\"\\nProcesando {len(all_texts)} diálogos (todo el dataset)...\")\n",
    "elmo_embeddings = get_elmo_embeddings(all_texts, batch_size=16)  # batch_size ajustable según RAM\n",
    "\n",
    "print(f\"\\nShape de embeddings: {elmo_embeddings.shape}\")\n",
    "print(f\"Dimensión por embedding: {elmo_embeddings.shape[1]}\")\n",
    "\n",
    "# Guardar embeddings\n",
    "embedding_data = {\n",
    "    'indices': list(range(len(all_texts))),\n",
    "    'embeddings': elmo_embeddings,\n",
    "    'shape': elmo_embeddings.shape,\n",
    "    'model_name': 'ELMo (TensorFlow Hub)',\n",
    "    'embedding_dim': elmo_embeddings.shape[1]\n",
    "}\n",
    "\n",
    "with open('../processed/elmo_embeddings_full.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_data, f)\n",
    "\n",
    "# Guardar también en tsv, para visualizarlo en embedding projector de tensorflow \n",
    "np.savetxt(\"../embedding_projector/elmo_embeddings_tsv.tsv\", elmo_embeddings, delimiter=\"\\t\")\n",
    "\n",
    "print(f\"\\nEmbeddings ELMo guardados en: processed/elmo_embeddings_full.pkl y embedding_projector/elmo_embeddings_tsv\")\n",
    "print(f\"  Total de diálogos procesados: {len(all_texts)}\")\n",
    "print(f\"  Dimensión: {elmo_embeddings.shape[1]}\")\n",
    "\n",
    "# Añadir referencias al DataFrame\n",
    "df['has_elmo_embedding'] = True\n",
    "df['elmo_embedding_idx'] = list(range(len(all_texts)))\n",
    "\n",
    "print(\"\\nReferencias añadidas al DataFrame para todos los diálogos\")\n",
    "\n",
    "# Ejemplo de similitud para los primeros 5\n",
    "print(\"\\nEJEMPLO: Similitudes ELMo entre primeros 5 diálogos\")\n",
    "sim_matrix = cosine_similarity(elmo_embeddings[:5])\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        print(f\"Diálogo {i} vs Diálogo {j}: similitud = {sim_matrix[i,j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0aac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata guardada en: processed/elmo_metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.DataFrame({\n",
    "    \"id\": df.index,\n",
    "    \"speaker\": df[\"dialogue\"].str.extract(r'^(Doctor|Patient|Guest_family|Guest_clinician|Doctor_2)', expand=False),\n",
    "    \"text_clean\": df[\"dialog_clean\"].str[:120],\n",
    "    \"section\": df[\"section_text_clean\"].str[:120]\n",
    "})\n",
    "\n",
    "metadata_df.to_csv(\"../embedding_projector/elmo_metadata.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"\\nMetadata guardada en: embedding_projector/elmo_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d37ecd",
   "metadata": {},
   "source": [
    "### BERT / BIO/ClinicalBERT (huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff178d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando 1201 diálogos con BioClinicalBERT...\n",
      "\n",
      "Cargando modelo: emilyalsentzer/Bio_ClinicalBERT\n",
      "  Procesados 80/1201\n",
      "  Procesados 160/1201\n",
      "  Procesados 240/1201\n",
      "  Procesados 320/1201\n",
      "  Procesados 400/1201\n",
      "  Procesados 480/1201\n",
      "  Procesados 560/1201\n",
      "  Procesados 640/1201\n",
      "  Procesados 720/1201\n",
      "  Procesados 800/1201\n",
      "  Procesados 880/1201\n",
      "  Procesados 960/1201\n",
      "  Procesados 1040/1201\n",
      "  Procesados 1120/1201\n",
      "  Procesados 1200/1201\n",
      "\n",
      "Shape de embeddings: (1201, 768)\n",
      "Dimensión por embedding: 768\n",
      "\n",
      "Embeddings BioClinicalBERT guardados en: processed/clinical_bert_embeddings_full y processed/embeddings_tsv\n",
      "\n",
      "Referencias añadidas al DataFrame para todos los diálogos\n",
      "\n",
      "EJEMPLO: Similitudes entre primeros 5 diálogos\n",
      "Diálogo 0 vs Diálogo 1: similitud = 0.8885\n",
      "Diálogo 0 vs Diálogo 2: similitud = 0.9338\n",
      "Diálogo 0 vs Diálogo 3: similitud = 0.8856\n",
      "Diálogo 0 vs Diálogo 4: similitud = 0.8332\n",
      "Diálogo 1 vs Diálogo 2: similitud = 0.9424\n",
      "Diálogo 1 vs Diálogo 3: similitud = 0.9173\n",
      "Diálogo 1 vs Diálogo 4: similitud = 0.8867\n",
      "Diálogo 2 vs Diálogo 3: similitud = 0.9186\n",
      "Diálogo 2 vs Diálogo 4: similitud = 0.8905\n",
      "Diálogo 3 vs Diálogo 4: similitud = 0.9085\n"
     ]
    }
   ],
   "source": [
    "def get_bert_embeddings(texts, model_name, max_length=512, batch_size=8):\n",
    "    \"\"\"\n",
    "    Extrae embeddings de BERT para todos los textos.\n",
    "    Usa el token [CLS] como representación de la secuencia completa.\n",
    "    \"\"\"\n",
    "    print(f\"\\nCargando modelo: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        \n",
    "        encoded = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            embeddings.extend(cls_embeddings)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Procesados {min(i+batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Modelo BioClinicalBERT preentrenado en textos clínicos\n",
    "selected_model = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "\n",
    "# Procesar TODO el dataset\n",
    "all_texts_bert = df['dialog_clean_clinicBERT'].tolist()\n",
    "\n",
    "print(f\"\\nProcesando {len(all_texts_bert)} diálogos con BioClinicalBERT...\")\n",
    "bert_embeddings = get_bert_embeddings(all_texts_bert, selected_model, batch_size=8)\n",
    "\n",
    "print(f\"\\nShape de embeddings: {bert_embeddings.shape}\")\n",
    "print(f\"Dimensión por embedding: {bert_embeddings.shape[1]}\")\n",
    "\n",
    "# Guardar embeddings en pickle\n",
    "embedding_data = {\n",
    "    'indices': list(range(len(all_texts_bert))),\n",
    "    'embeddings': bert_embeddings,\n",
    "    'shape': bert_embeddings.shape,\n",
    "    'model_name': selected_model,\n",
    "    'embedding_dim': bert_embeddings.shape[1]}\n",
    "\n",
    "with open('../processed/clinical_bert_embeddings_full.pkl', 'wb') as f:\n",
    "    pickle.dump(embedding_data, f)\n",
    "\n",
    "# Guardar también en tsv, para visualizarlo en embedding projector de tensorflow \n",
    "np.savetxt(\"../embedding_projector/clinical_bert_embeddings_tsv.tsv\", bert_embeddings, delimiter=\"\\t\")\n",
    "\n",
    "print(f\"\\nEmbeddings BioClinicalBERT guardados en: processed/clinical_bert_embeddings_full y embedding_projector/clinical_bert_embeddings_tsv\")\n",
    "\n",
    "# Añadir referencias al DataFrame\n",
    "df['has_bert_embedding'] = True\n",
    "df['bert_embedding_idx'] = list(range(len(all_texts_bert)))\n",
    "\n",
    "print(\"\\nReferencias añadidas al DataFrame para todos los diálogos\")\n",
    "\n",
    "# Ejemplo de similitud para los primeros 5\n",
    "print(\"\\nEJEMPLO: Similitudes entre primeros 5 diálogos\")\n",
    "sim_matrix = cosine_similarity(bert_embeddings[:5])\n",
    "for i in range(5):\n",
    "    for j in range(i+1, 5):\n",
    "        print(f\"Diálogo {i} vs Diálogo {j}: similitud = {sim_matrix[i,j]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c55f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metadata guardada en: processed/clinical_bert_metadata.tsv\n"
     ]
    }
   ],
   "source": [
    "metadata_df = pd.DataFrame({\n",
    "    \"id\": df.index,\n",
    "    \"text_bert\": df[\"dialog_clean_clinicBERT\"].str[:120]\n",
    "})\n",
    "\n",
    "metadata_df.to_csv(\"../embedding_projector/clinical_bert_metadata.tsv\", sep=\"\\t\", index=False)\n",
    "print(\"\\nMetadata guardada en: embedding_projector/clinical_bert_metadata.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97753a2",
   "metadata": {},
   "source": [
    "Hemos guardado los embeddings en formato pickle y tsv. \n",
    "\n",
    "Para visualizarlos, si runneas \"tensorboard --logdir embedding_projector\", y abres http://localhost:6006, puedes visualizar los embeddings contextuales.  Dejamos el comando preparado aquí abajo por si quieres verlo aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558b962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 7824), started 0:00:23 ago. (Use '!kill 7824' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e894f21e7cfacf38\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e894f21e7cfacf38\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ../embedding_projector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
