{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in a fresh environment)\n",
    "# %pip install -q transformers datasets evaluate accelerate sentencepiece sacrebleu rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b25d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.makedirs('processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba739817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV and prepare text pairs\n",
    "df = pd.read_csv('dataset/MTS-Dialog-TrainingSet.csv')\n",
    "print('Rows:', len(df))\n",
    "\n",
    "# Use cleaned columns if available, otherwise fallback to raw dialogue/section_text\n",
    "if 'dialog_clean' in df.columns:\n",
    "    inputs = df['dialog_clean'].fillna('').astype(str).tolist()\n",
    "else:\n",
    "    inputs = df['dialogue'].fillna('').astype(str).tolist()\n",
    "\n",
    "if 'section_text_clean' in df.columns:\n",
    "    targets = df['section_text_clean'].fillna('').astype(str).tolist()\n",
    "else:\n",
    "    targets = df['section_text'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Quick sanity: filter out empty pairs\n",
    "pairs = [(i, t) for i, t in zip(inputs, targets) if str(i).strip() and str(t).strip()]\n",
    "print(f'Usable pairs: {len(pairs)}')\n",
    "inputs, targets = zip(*pairs)\n",
    "\n",
    "# Train/val split\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(list(inputs), list(targets), test_size=0.12, random_state=SEED)\n",
    "print('Train size:', len(train_inputs), 'Val size:', len(val_inputs))\n",
    "\n",
    "raw_train = Dataset.from_dict({'dialogue': train_inputs, 'summary': train_targets})\n",
    "raw_val = Dataset.from_dict({'dialogue': val_inputs, 'summary': val_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6bd29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model & tokenizer selection\n",
    "model_name = 't5-small'  # change to 't5-base' or other model if you have resources\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Prefix used by T5 for summarization tasks\n",
    "prefix = 'summarize: '\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples['dialogue']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "    # Tokenize targets with the `text_target` argument (newer tokenizers) or encode normally\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Map the datasets\n",
    "train_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "val_dataset = raw_val.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator and metric\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects newline separated sentences\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=['rouge1','rouge2','rougeL'])\n",
    "    # extract mid F1 scores and return\n",
    "    return {k: round(v.mid.fmeasure, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feac25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments - tune to your hardware\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='processed/t5_summarization',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,  # set True if you have a GPU with mixed precision\n",
    "    seed=SEED,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='rouge1',\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training (uncomment to run)\n",
    "# trainer.train()\n",
    "# trainer.save_model('processed/t5_summarization_best')\n",
    "# tokenizer.save_pretrained('processed/t5_summarization_best')\n",
    "\n",
    "print('Trainer is ready. To launch training uncomment trainer.train()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209172f",
   "metadata": {},
   "source": [
    "## Inference example\n",
    "After training and saving the model, you can run the cell below to load the trained model and generate summaries for new dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3951d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and tokenizer (if saved)\n",
    "# model = T5ForConditionalGeneration.from_pretrained('processed/t5_summarization_best')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('processed/t5_summarization_best')\n",
    "\n",
    "def generate_summary(text, max_length=120, num_beams=4):\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_input_length).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=True)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Example (replace with a real dialogue)\n",
    "# sample_dialog = df['dialog_clean'].iloc[0]\n",
    "# print('INPUT:', sample_dialog)\n",
    "# print('GENERATED SUMMARY:', generate_summary(sample_dialog))\n",
    "print('Inference cell ready. Load the saved model and run generate_summary(...)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2896e92",
   "metadata": {},
   "source": [
    "## Tips & Next steps\n",
    "- If dialogues are long, consider a model that supports longer inputs (LongT5, Longformer + BART) or a hierarchical approach (chunk + summarize + merge).\n",
    "- Use mixed precision (`fp16=True`) if GPU supports it to speed up training.\n",
    "- Monitor validation ROUGE and save best checkpoint; early stopping helps avoid overfitting.\n",
    "- Optionally pre-train (continued pretraining) the LM on your corpus before fine-tuning for summarization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
