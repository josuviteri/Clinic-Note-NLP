{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe3b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in a fresh environment)\n",
    "# %pip install -q transformers datasets evaluate accelerate sentencepiece sacrebleu rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf5a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: transformers in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (4.57.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (2.3.5)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: certifi in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1.0.0->datasets)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: xxhash, sniffio, sentencepiece, pyarrow, propcache, multidict, h11, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, datasets, evaluate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [evaluate]/20\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 attrs-25.4.0 datasets-4.4.1 dill-0.4.0 evaluate-0.4.6 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 sentencepiece-0.2.1 sniffio-1.3.1 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~umpy (/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U datasets evaluate transformers sentencepiece \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b25d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "os.makedirs('processed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba739817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1201\n",
      "Usable pairs: 1201\n",
      "Train size: 1056 Val size: 145\n"
     ]
    }
   ],
   "source": [
    "# Load CSV and prepare text pairs\n",
    "df = pd.read_csv('../dataset/MTS-Dialog-TrainingSet.csv')\n",
    "print('Rows:', len(df))\n",
    "\n",
    "# Use cleaned columns if available, otherwise fallback to raw dialogue/section_text\n",
    "if 'dialog_clean' in df.columns:\n",
    "    inputs = df['dialog_clean'].fillna('').astype(str).tolist()\n",
    "else:\n",
    "    inputs = df['dialogue'].fillna('').astype(str).tolist()\n",
    "\n",
    "if 'section_text_clean' in df.columns:\n",
    "    targets = df['section_text_clean'].fillna('').astype(str).tolist()\n",
    "else:\n",
    "    targets = df['section_text'].fillna('').astype(str).tolist()\n",
    "\n",
    "# Quick sanity: filter out empty pairs\n",
    "pairs = [(i, t) for i, t in zip(inputs, targets) if str(i).strip() and str(t).strip()]\n",
    "print(f'Usable pairs: {len(pairs)}')\n",
    "inputs, targets = zip(*pairs)\n",
    "\n",
    "# Train/val split\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(list(inputs), list(targets), test_size=0.12, random_state=SEED)\n",
    "print('Train size:', len(train_inputs), 'Val size:', len(val_inputs))\n",
    "\n",
    "raw_train = Dataset.from_dict({'dialogue': train_inputs, 'summary': train_targets})\n",
    "raw_val = Dataset.from_dict({'dialogue': val_inputs, 'summary': val_targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df6bd29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1056 [00:00<?, ? examples/s]/home/bnnyrabbit/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4034: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1056/1056 [00:00<00:00, 3380.32 examples/s]\n",
      "Map: 100%|██████████| 145/145 [00:00<00:00, 3077.85 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1056\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 145\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model & tokenizer selection\n",
    "model_name = 't5-small'  # change to 't5-base' or other model if you have resources\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Prefix used by T5 for summarization tasks\n",
    "prefix = 'summarize: '\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples['dialogue']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "    # Tokenize targets with the `text_target` argument (newer tokenizers) or encode normally\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['summary'], max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# Map the datasets\n",
    "train_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "val_dataset = raw_val.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary'])\n",
    "\n",
    "print(train_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d17a4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator and metric\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Rouge expects newline separated sentences\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=['rouge1','rouge2','rougeL'])\n",
    "    # extract mid F1 scores and return\n",
    "    return {k: round(v.mid.fmeasure, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feac25d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training arguments - tune to your hardware\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprocessed/t5_summarization\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#evaluation_strategy='epoch',\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msteps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# set True if you have a GPU with mixed precision\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrouge1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m trainer = Seq2SeqTrainer(\n\u001b[32m     20\u001b[39m     model=model,\n\u001b[32m     21\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Start training (uncomment to run)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# trainer.train()\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# trainer.save_model('processed/t5_summarization_best')\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# tokenizer.save_pretrained('processed/t5_summarization_best')\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:140\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Uni/4.o/NLP/.env_nlp/lib/python3.12/site-packages/transformers/training_args.py:1689\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.load_best_model_at_end \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_strategy != SaveStrategy.BEST:\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy != \u001b[38;5;28mself\u001b[39m.save_strategy:\n\u001b[32m-> \u001b[39m\u001b[32m1689\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m--load_best_model_at_end requires the save and eval strategy to match, but found\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Evaluation \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1691\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mstrategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.eval_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- Save strategy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.save_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1692\u001b[39m         )\n\u001b[32m   1693\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_strategy == IntervalStrategy.STEPS \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps % \u001b[38;5;28mself\u001b[39m.eval_steps != \u001b[32m0\u001b[39m:\n\u001b[32m   1694\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.eval_steps < \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_steps < \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: IntervalStrategy.NO\n- Save strategy: SaveStrategy.EPOCH"
     ]
    }
   ],
   "source": [
    "# Training arguments - tune to your hardware\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='processed/t5_summarization',\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=200,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    "    fp16=False,  # set True if you have a GPU with mixed precision\n",
    "    seed=SEED,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='rouge1',\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training (uncomment to run)\n",
    "# trainer.train()\n",
    "# trainer.save_model('processed/t5_summarization_best')\n",
    "# tokenizer.save_pretrained('processed/t5_summarization_best')\n",
    "\n",
    "print('Trainer is ready. To launch training uncomment trainer.train()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5209172f",
   "metadata": {},
   "source": [
    "## Inference example\n",
    "After training and saving the model, you can run the cell below to load the trained model and generate summaries for new dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3951d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model and tokenizer (if saved)\n",
    "# model = T5ForConditionalGeneration.from_pretrained('processed/t5_summarization_best')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('processed/t5_summarization_best')\n",
    "\n",
    "def generate_summary(text, max_length=120, num_beams=4):\n",
    "    input_text = prefix + text\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', truncation=True, max_length=max_input_length).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_length=max_length, num_beams=num_beams, early_stopping=True)\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "# Example (replace with a real dialogue)\n",
    "# sample_dialog = df['dialog_clean'].iloc[0]\n",
    "# print('INPUT:', sample_dialog)\n",
    "# print('GENERATED SUMMARY:', generate_summary(sample_dialog))\n",
    "print('Inference cell ready. Load the saved model and run generate_summary(...)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2896e92",
   "metadata": {},
   "source": [
    "## Tips & Next steps\n",
    "- If dialogues are long, consider a model that supports longer inputs (LongT5, Longformer + BART) or a hierarchical approach (chunk + summarize + merge).\n",
    "- Use mixed precision (`fp16=True`) if GPU supports it to speed up training.\n",
    "- Monitor validation ROUGE and save best checkpoint; early stopping helps avoid overfitting.\n",
    "- Optionally pre-train (continued pretraining) the LM on your corpus before fine-tuning for summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_nlp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
