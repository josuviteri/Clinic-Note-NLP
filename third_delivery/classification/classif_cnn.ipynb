{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f1b0b5",
   "metadata": {},
   "source": [
    "Deep learning approaches:\n",
    "\n",
    "Fine‑tune Bio_ClinicalBERT (classification head):\n",
    "For long dialogues >512 tokens: chunk, get CLS for each chunk, mean/max pool, then classify.\n",
    "\n",
    "ELMo + BiLSTM + Attention classifier.\n",
    "Fast baseline transformer: DistilBERT (clinical domain variant if available) fine‑tune.\n",
    "\n",
    "CNN text classifier (Kim CNN) with static + fine‑tunable embeddings (FastText init).\n",
    "Hierarchical model (utterance encoder → dialogue encoder) if time permits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7df06",
   "metadata": {},
   "source": [
    "## FIRST APPROACH: Fine‑tune Bio_ClinicalBERT (classification head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b448da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e56da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../dataset/MTS-Dialog-TrainingSet.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723278b",
   "metadata": {},
   "source": [
    "# FINETUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9729c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 960/960 [00:00<00:00, 3864.05 examples/s]\n",
      "Map: 100%|██████████| 241/241 [00:00<00:00, 3535.02 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 01:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.931700</td>\n",
       "      <td>1.643910</td>\n",
       "      <td>0.634855</td>\n",
       "      <td>0.262969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.277400</td>\n",
       "      <td>1.200764</td>\n",
       "      <td>0.759336</td>\n",
       "      <td>0.339809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.106800</td>\n",
       "      <td>1.117554</td>\n",
       "      <td>0.771784</td>\n",
       "      <td>0.345453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31/31 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1175535917282104, 'eval_accuracy': 0.7717842323651453, 'eval_f1_macro': 0.34545303347045025, 'eval_runtime': 1.8796, 'eval_samples_per_second': 128.218, 'eval_steps_per_second': 16.493, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"../../dataset/MTS-Dialog-TrainingSet.csv\")\n",
    "\n",
    "# Minimal preprocessing for BERT (keep case, minimal cleaning)\n",
    "def normalize_for_bert(s):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    # Remove speaker labels (BERT will learn from context, not explicit tags)\n",
    "    s = re.sub(r'\\b(Doctor|Doctor_2|Patient|Guest_family(_\\d)?|Guest_clinician)[:\\-]\\s*', '', s, flags=re.I)\n",
    "    # Keep punctuation, contractions, case as-is\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['text_for_bert'] = df['dialogue'].apply(normalize_for_bert)\n",
    "\n",
    "# Prepare data\n",
    "X = df['text_for_bert']\n",
    "y = df['section_header']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(le.classes_),\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({'text': X_train.tolist(), 'label': y_train.tolist()})\n",
    "test_dataset = Dataset.from_dict({'text': X_test.tolist(), 'label': y_test.tolist()})\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro')\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_clinicalbert',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    seed=42,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained('./finetuned_clinicalbert')\n",
    "tokenizer.save_pretrained('./finetuned_clinicalbert')\n",
    "\n",
    "# Save label encoder\n",
    "import pickle\n",
    "with open('./finetuned_clinicalbert/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(le, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f8300",
   "metadata": {},
   "source": [
    "INFERENCE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0190c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: FAM/SOCHX, Confidence: 0.138\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9247ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: GENHX | Confidence: 0.924\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import pickle\n",
    "import re, unicodedata\n",
    "\n",
    "# Minimal preprocessing (same as training)\n",
    "def normalize_for_bert(s):\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = re.sub(r'\\b(Doctor|Doctor_2|Patient|Guest_family(_\\d)?|Guest_clinician)[:\\-]\\s*', '', s, flags=re.I)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# Load label encoder and model\n",
    "with open('./finetuned_clinicalbert/label_encoder.pkl', 'rb') as f:\n",
    "    le = pickle.load(f)\n",
    "\n",
    "clf = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"./finetuned_clinicalbert\",\n",
    "    tokenizer=\"./finetuned_clinicalbert\",\n",
    "    device=0  # use -1 for CPU\n",
    ")\n",
    "\n",
    "# Load a validation sample\n",
    "df_val = pd.read_csv(\"../../dataset/MTS-Dialog-ValidationSet.csv\")\n",
    "text = normalize_for_bert(df_val.loc[0, \"dialogue\"])  # any row from validation set\n",
    "\n",
    "# Predict\n",
    "out = clf(text)[0]\n",
    "pred_label = le.inverse_transform([int(out['label'].split('_')[-1])])[0]\n",
    "print(f\"Predicted: {pred_label} | Confidence: {out['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0debb",
   "metadata": {},
   "source": [
    "In order to use any text written by us, we can use the following script\n",
    "\n",
    "The text is an example and nneds to be written as the model expects, won't work if not like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fa7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: FAM/SOCHX, Confidence: 0.138\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Patient reports chest pain for 3 days...\"\n",
    "result = classifier(sample_text)\n",
    "predicted_label = le.inverse_transform([int(result[0]['label'].split('_')[-1])])[0]\n",
    "print(f\"Predicted: {predicted_label}, Confidence: {result[0]['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9e9ec",
   "metadata": {},
   "source": [
    "## SECOND APPROACH: ELMo + BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87adc6b5",
   "metadata": {},
   "source": [
    "    ELMo + BiLSTM outline\n",
    "We already extract ELMo sentence vectors, for sequence modeling we’d keep token-level embeddings and feed them to a BiLSTM + attention before a dense softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_nlp (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
