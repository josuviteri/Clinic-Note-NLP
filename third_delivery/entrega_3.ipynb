{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfe442c",
   "metadata": {},
   "source": [
    "### Tareas específicas de la entrega 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5da61f",
   "metadata": {},
   "source": [
    "Hemos puesto en esta primera celda los requisitos, que son los imports y el proceso de normalización de la sección 1, para seguir con las tareas de esta entrega. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed67bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import pickle\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP y embeddings\n",
    "import spacy\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "df = pd.read_csv(\"../dataset/MTS-Dialog-TrainingSet.csv\")\n",
    "\n",
    "contraction_map = {\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"here's\": \"here is\"\n",
    "}\n",
    "\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = text.lower()\n",
    "    for c, repl in contraction_map.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(c) + r\"\\b\", repl, text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(s, lowercase=True):\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "\n",
    "    # Normalizar unicode\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "\n",
    "    # Marcadores de quién habla -> usar tokens temporales\n",
    "    s = re.sub(r'\\bDoctor[:\\-]\\s*', ' __doc__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bDoctor_2[:\\-]\\s*', ' __doc2__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bPatient[:\\-]\\s*', ' __pat__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family[:\\-]\\s*', ' __fam__ ', s, flags=re.I)\n",
    "    s = re.sub(r'\\bGuest_family_1[:\\-]\\s*', ' __fam__ ', s, flags=re.I) \n",
    "    #Si hay dos visitantes, el primero cambia de guest_family a guest_family_1, vamos a igualarlos, esté solo o no siempre será <FAMILY>\n",
    "    s = re.sub(r'\\bGuest_family_2[:\\-]\\s*', ' __fam2__ ', s, flags=re.I) \n",
    "    s = re.sub(r'\\bGuest_clinician[:\\-]\\s*', ' __clin__ ', s, flags=re.I)\n",
    "    \n",
    "    # Expand contractions (suponiendo que tienes esta función)\n",
    "    s = expand_contractions(s)\n",
    "\n",
    "    # Separar puntuación\n",
    "    s = re.sub(r'([.,!?;:()\"\\[\\]])', r' \\1 ', s)\n",
    "\n",
    "    # Reducir espacios\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "    # Lowercase todo excepto los tags\n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "\n",
    "    # Restaurar los tags en mayúsculas\n",
    "    s = s.replace('__doc__', '<DOC>')\n",
    "    s = s.replace('__doc2__', '<DOC2>')\n",
    "    s = s.replace('__pat__', '<PAT>')\n",
    "    s = s.replace('__fam__', '<FAMILY>')\n",
    "    s = s.replace('__fam2__', '<FAMILY2>')\n",
    "    s = s.replace('__clin__', '<CLIN>')\n",
    "\n",
    "    return s\n",
    " \n",
    "\n",
    "# Versión para ELMo (lowercase)\n",
    "df['dialog_clean'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=True))\n",
    "\n",
    "# Versión para BIO/ClinicalBERT (manteniendo mayúsculas)\n",
    "df['dialog_clean_clinicBERT'] = df['dialogue'].apply(lambda x: normalize_text(x, lowercase=False))\n",
    "\n",
    "# Los resúmenes\n",
    "df['section_text_clean'] = df['section_text'].apply(lambda x: normalize_text(x, lowercase=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8307902",
   "metadata": {},
   "source": [
    "    Técnica de Bag of Words en Machine Learning\n",
    "Usaremos CountVectorizer de scikit-learn para transformar el corpus de textos en una matriz de frecuencias de palabras, creando una representación dispersa donde cada palabra es una columna y cada documento una fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words (unigrams + bigrams)\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_bow = cv.fit_transform(df['dialog_clean'])\n",
    "\n",
    "# TF-IDF\n",
    "tfv = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_tfidf = tfv.fit_transform(df['dialog_clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339c38c",
   "metadata": {},
   "source": [
    "    Modelo de Bag of Words\n",
    "Este método cuantifica la presencia y frecuencia de las palabras en los textos, generando vectores dispersos de alta dimensión. Al transformar los datos de texto en características numéricas, se facilitará su uso en algoritmos de aprendizaje automático, permitiendo realizar tareas como clasificación, análisis de sentimientos y recomendaciones, aunque sin capturar relaciones semánticas complejas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
